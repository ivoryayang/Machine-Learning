{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTWSmOwtddM0"
   },
   "source": [
    "# HW4 Multi-Class Classification (Due: Thursday, February 29, 2024, 11:59 PM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP2DeJOGddM2"
   },
   "source": [
    "####❗Please submit this notebook (`.ipynb`) with your solutions. The solutions must include the **code**, **explanations** (e.g. *comments*, *figure title, axis labels, and legend*), and the **output of all the cells**. Submitting your solutions without them will lead to `a deduction of points`.\n",
    "\n",
    "####❗Also, note that you should **cite all the references** you used under each question. Proper referencing is essential for academic integrity, giving credit to original authors, avoiding plagiarism, and providing a traceable path for verification. `Please check the course syllabus for more details about academic integrity`.\n",
    "\n",
    "####❗Note that you are *not* allowed to use sklearn unless specified by the question.\n",
    "\n",
    "####❗Please note that the bonus points will be applied to your overall percentage of the course.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D1KyDVYddM3"
   },
   "source": [
    "#### **Q0)** [[0 point]](https://) While you are allowed to discuss homework assignments, it is essential that you write down your code and solutions yourself. If you have discussed the homework with other students, please mention their names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGKyrB_1KV8J"
   },
   "source": [
    "\\##Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0Yhv6bTddM4"
   },
   "source": [
    "####Please run the cell below to import libraries needed for this HW. Please use the autograd numpy, otherwise you will have issues. Please remember to always use the np library for mathematical functions (e.g., np.log, np.exp, np.sum, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zu-X5y1jddM4"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n6XqvuMOaBw"
   },
   "source": [
    "#### Please load the MNIST digits classification dataset by running the code below. There are 70,000 samples in the dataset where each sample is represented by a 784-dimensional feature vector. (Reference: https://www.openml.org/d/554)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FDTG70hl4oWt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "['5' '0' '4' ... '4' '5' '6']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMvPHxUZOubj"
   },
   "source": [
    "#### Run the code below to divide the data into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FHLg-QBA0kZA"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=10000, test_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naA_gNyANCUh"
   },
   "source": [
    "# $k$ Nearest Neighbors ($k$-NN) Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atvctf4WP22O"
   },
   "source": [
    "#### **Q1**) [[30 points]](https://) Implement a $k$-Nearest Neighbors classifier *from scratch*. You are free to code this up in your preferred style. However, make sure that your implementation includes a function that takes an array of feature vectors for the training set, the corresponding class labels, an array of feature vectors for the test set, and the number of nearest neighbors, $k$. The function should return an array of predicted labels for the input test data. Make sure you explain in detail about your implmenetaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TXMQiJ9i608R"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def knn_classifier(X_train, y_train, X_test, k):\n",
    "    \n",
    "    # Function to compute Euclidean distance between two vectors\n",
    "    def euclidean_distance(x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    # Function to find k-nearest neighbors for a single test sample\n",
    "    def get_neighbors(x_test):\n",
    "        \n",
    "        # Calculate Euclidean distances between the test sample and all training samples\n",
    "        distances = [euclidean_distance(x_test, x_train) for x_train in X_train]\n",
    "        \n",
    "        # Sort distances in ascending order and select the first 'k' (nearest neighbors)\n",
    "        indices = np.argsort(distances)[:k]\n",
    "        return indices\n",
    "\n",
    "    # Function to perform majority voting among neighbors\n",
    "    def majority_vote(neighbors):\n",
    "\n",
    "        # Count occurrences of each label in the neighbors\n",
    "        counts = np.bincount(y_train[neighbors])\n",
    "\n",
    "        # Return the label with the highest count (majority vote)\n",
    "        return np.argmax(counts)\n",
    "\n",
    "    # Predict the class labels for the test set\n",
    "    y_pred = []\n",
    "    for x_test in X_test:\n",
    "        neighbors = get_neighbors(x_test)\n",
    "        predicted_label = majority_vote(neighbors)\n",
    "        y_pred.append(predicted_label)\n",
    "\n",
    "    # Return array of predicted labels for the input test data\n",
    "    return np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FePjGak1QzuP"
   },
   "source": [
    "#### **Q2**) [[15 points]](https://) Implement a function to calcluate the accuracy for a multi-class classification model *from scratch*. Given predicted labels and true labels, your function should return the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DYPjwEw6_xMj"
   },
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    \n",
    "    # Ensure the lengths of predicted and true labels are the same\n",
    "    if len(y_pred) != len(y_true):\n",
    "        raise ValueError(\"Length mismatch.\")\n",
    "\n",
    "    # Count the number of correctly classified samples\n",
    "    correct_count = sum(1 for pred, true in zip(y_pred, y_true) if pred == true)\n",
    "\n",
    "    # Calculate accuracy as the ratio of correctly classified samples to total samples\n",
    "    accuracy = correct_count / len(y_true)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qTnv2CpRKdu"
   },
   "source": [
    "#### **Q3**) [[5 points]](https://) Given $k$=3, calculate the accuracy of the test set using the function implemented in Q2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VuQqKi-LMZH5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9565\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_train is initially in string format\n",
    "unique_labels_train = np.unique(y_train)\n",
    "label_mapping_train = {label: idx for idx, label in enumerate(unique_labels_train)}\n",
    "y_train_encoded = np.array([label_mapping_train[label] for label in y_train])\n",
    "\n",
    "# Get predicted labels using the k-NN classifier\n",
    "k = 3\n",
    "y_pred = knn_classifier(X_train, y_train_encoded, X_test, k)\n",
    "\n",
    "# Assuming y_test is initially in string format\n",
    "unique_labels_test = np.unique(y_test)\n",
    "label_mapping_test = {label: idx for idx, label in enumerate(unique_labels_test)}\n",
    "\n",
    "# Check if there are any labels in y_test that were not present in y_train\n",
    "new_labels = set(y_test) - set(label_mapping_train.keys())\n",
    "if new_labels:\n",
    "    print(f\"Warning: New labels found in y_test: {new_labels}\")\n",
    "\n",
    "# Convert y_test to encoded labels\n",
    "y_test_encoded = np.array([label_mapping_train[label] if label in label_mapping_train else label_mapping_test[label] for label in y_test])\n",
    "\n",
    "# Calculate accuracy using the calculate_accuracy function\n",
    "accuracy = calculate_accuracy(y_pred, y_test_encoded)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPtuZnmeessH"
   },
   "source": [
    "# Naive Bayes Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diy76183O7uU"
   },
   "source": [
    "#### **Q4**) [[60 points]](https://) Implement both a Multinomial Naive Bayes classifier with Laplace smoothing [30 points] and a Gaussian Naive Bayes classifier [30 points] *from scratch*. To prevent everything from becoming zeros, consider adding logs instead of multiplying. You are free to code this up in your preferred style, but you should explain in detail about your implmenetaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    \n",
    "    def __init__(self, smoothing=1.0):\n",
    "        # Smoothing coefficient for Laplace smoothing\n",
    "        self.smoothing = smoothing\n",
    "        # Matrix of feature weights\n",
    "        self.weights = None\n",
    "        # Log priors for each class\n",
    "        self.priors = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # One-hot encoding for Y\n",
    "        K = len(set(Y)) # Number of classes\n",
    "        N = len(Y) # Number of samples\n",
    "        labels = Y\n",
    "        Y_one_hot = np.zeros((N, K))\n",
    "        Y_one_hot[np.arange(N), labels] = 1\n",
    "\n",
    "        # Calculate log-weights and log-priors\n",
    "        self.weights = np.log(X.T.dot(Y_one_hot) + self.smoothing) - np.log((X.T.dot(Y_one_hot) + self.smoothing).sum(axis=0))\n",
    "        self.priors = np.log(Y_one_hot.sum(axis=0)) - np.log(Y_one_hot.sum(axis=0).sum())\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        # Accuracy of model on given data\n",
    "        return np.mean(self.predict(X) == Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predicted class labels\n",
    "        return np.argmax(X.dot(self.weights) + self.priors, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def __init__(self, epsilon=1e-1):\n",
    "        # Constructor to initialize the Gaussian Naive Bayes model\n",
    "        self.class_means = None\n",
    "        self.class_variances = None\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Fit the model to the training data\n",
    "        unique_classes = np.unique(y_train)\n",
    "\n",
    "        # Store mean and variance for each feature in each class\n",
    "        self.class_means = {}\n",
    "        self.class_variances = {}\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            # Extract features corresponding to the current class\n",
    "            cls_indices = (y_train == cls)\n",
    "            cls_features = X_train[cls_indices, :]\n",
    "\n",
    "            # Calculate mean and variance for each feature\n",
    "            class_mean = np.mean(cls_features, axis=0)\n",
    "            class_variance = np.var(cls_features, axis=0) + self.epsilon\n",
    "\n",
    "            # Store mean and variance for the current class\n",
    "            self.class_means[cls] = class_mean\n",
    "            self.class_variances[cls] = class_variance\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Make predictions for the input data\n",
    "        predictions = []\n",
    "\n",
    "        for sample in X_test:\n",
    "            class_scores = []\n",
    "\n",
    "            for cls, mean in self.class_means.items():\n",
    "                # Calculate log-likelihood using Gaussian distribution formula\n",
    "                log_likelihood = -0.5 * np.sum(np.log(2 * np.pi * self.class_variances[cls]))\n",
    "                log_likelihood -= 0.5 * np.sum(((sample - mean) ** 2) / (self.class_variances[cls]))\n",
    "                class_scores.append(log_likelihood)\n",
    "\n",
    "            # Predict the class with the highest log-likelihood\n",
    "            predictions.append(np.argmax(class_scores))\n",
    "\n",
    "        return np.array(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idG1RtujPTTa"
   },
   "source": [
    "#### **Q5**) [[10 points]](https://) Calculate the accuracy of the test set using both of the Naive Bayes classifiers you implemented in Q4. Use the function you implemented in Q2 to obtain the accuracies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X7vVHcs6Mb-a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Accuracy: 0.841\n",
      "Gaussian Naive Bayes Accuracy: 0.628\n"
     ]
    }
   ],
   "source": [
    "# Set up\n",
    "multinomial_nb = MultinomialNaiveBayes()\n",
    "y_train_int = y_train.astype(int)\n",
    "multinomial_nb.fit(X_train, y_train_int)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_multinomial_nb = multinomial_nb.predict(X_test)\n",
    "\n",
    "# Ensure predictions are integers\n",
    "y_test_encoded_multinomial = [np.int64(str_value) for str_value in y_test]\n",
    "\n",
    "# Calculate accuracy for Multinomial Naive Bayes\n",
    "accuracy_multinomial_nb = calculate_accuracy(y_pred_multinomial_nb, y_test_encoded_multinomial)\n",
    "print(f\"Multinomial Naive Bayes Accuracy:\", accuracy_multinomial_nb)\n",
    "\n",
    "# Instantiate Gaussian Naive Bayes\n",
    "gaussian_nb = GaussianNaiveBayes()\n",
    "gaussian_nb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions for Gaussian Naive Bayes\n",
    "y_pred_gaussian_nb = gaussian_nb.predict(X_test)\n",
    "\n",
    "# Ensure predictions are integers\n",
    "y_test_encoded_gaussian = [np.int64(str_value) for str_value in y_test]\n",
    "\n",
    "# Calculate accuracy for Gaussian Naive Bayes\n",
    "accuracy_gaussian_nb = calculate_accuracy(y_pred_gaussian_nb, y_test_encoded_gaussian)\n",
    "print(\"Gaussian Naive Bayes Accuracy:\", accuracy_gaussian_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZE_rteyUTjR"
   },
   "source": [
    "#### **Q6**) [[10 points]](https://) Compare the performance of the three classifiers ($k$-NN, Multinomial Naive Bayes, Gaussian Naive Bayes). Which classifier performs the best? Provide a detailed explanation of your conclusion. Make sure that the comparison is based on the same train/test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zjAE1YXUWhT"
   },
   "source": [
    "The k-NN classifier had an accuracy of 0.9565, the Multinomial Naive Bayes classifier had an accuracy of 0.841 and the Gaussian Naive Bayes classifier had an accuracy of 0.628. The k-NN classifier performs the best, based on the same train/test splits. This could be because the k-NN algorithm is able to capture complex relationships within the data by considering local patterns and nearest neighbors. However, Naive Bayes classifiers make strong independence assumptions. These may not hold in scenarios with intricate dependencies among features. In particular, Gaussian Naives Bayes assumes a Gaussian distribution for each class, and if the data deviates significantly from this assumption, its performance may suffer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgeKbUPNNj5L"
   },
   "source": [
    "# Support Vector Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Gwl49bpV7P9"
   },
   "source": [
    "#### **Q7**) [[***BONUS*** 0.5%]](https://) Train 10 one-versus-all binary classifiers using the soft-margin SVM cost function and the gradient descent function. You may need to revise your gradient descent function for this task. Randomly initialize all model parameters with a normal distribution having a mean of 0 and a standard deviation of 0.01. Then, train each classifier with `max_its=1000` and `alpha=0.0001`. Consider implementing a sampling strategy to deal with an imbalanced dataset. Set your regularization parameter to 1.0. Save the cost and weight history returned by the gradient descent function. Plot the cost history for the 10 binary classifiers into a single plot.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNjl_oPOMdJv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr8KK7IeWXfe"
   },
   "source": [
    "#### **Q8**) [[***BONUS*** 0.4%]](https://) Implement the one-versus-all multiway classifier using the binary classifiers you implemented above. You are free to code this up in your preferred style. However, make sure that your implementation includes a function that takes an array of feature vectors for the test set and an array of learned model parameters. This classifier should return an array of predicted labels for the input test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrAeXJ5IEuXN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rL7jHdlLXjZB"
   },
   "source": [
    "#### **Q9**) [[***BONUS*** 0.1%]](https://) Calculate the accuracy of the test set using your one-versus-all classifier implemented above. Use the function implemented in Q2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
